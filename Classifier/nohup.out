libdc1394 error: Failed to initialize libdc1394
I1126 19:38:40.256013 20696 caffe.cpp:218] Using GPUs 0
I1126 19:38:40.259811 20696 caffe.cpp:223] GPU 0: Tesla K80
I1126 19:38:40.710919 20696 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 2000
snapshot_prefix: "snapshots/classifier"
device_id: 0
net: "train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I1126 19:38:40.711078 20696 solver.cpp:87] Creating training net from net file: train_val.prototxt
I1126 19:38:40.711378 20696 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1126 19:38:40.711396 20696 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1126 19:38:40.711464 20696 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215689
  }
  data_param {
    source: "/home/styurin/DataSet/lmdb/train"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 20
    kernel_size: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "sigmoid1"
  type: "Sigmoid"
  bottom: "conv1"
  top: "sigmoid1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "sigmoid1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 40
    kernel_size: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "sigmoid2"
  type: "Sigmoid"
  bottom: "conv2"
  top: "sigmoid2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "sigmoid2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "fc1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "fc1"
  top: "score"
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
I1126 19:38:40.711531 20696 layer_factory.hpp:77] Creating layer data
I1126 19:38:40.711632 20696 db_lmdb.cpp:35] Opened lmdb /home/styurin/DataSet/lmdb/train
I1126 19:38:40.711664 20696 net.cpp:84] Creating Layer data
I1126 19:38:40.711674 20696 net.cpp:380] data -> data
I1126 19:38:40.711699 20696 net.cpp:380] data -> label
I1126 19:38:40.712610 20696 data_layer.cpp:45] output data size: 32,3,64,64
I1126 19:38:40.722575 20696 net.cpp:122] Setting up data
I1126 19:38:40.722597 20696 net.cpp:129] Top shape: 32 3 64 64 (393216)
I1126 19:38:40.722604 20696 net.cpp:129] Top shape: 32 (32)
I1126 19:38:40.722609 20696 net.cpp:137] Memory required for data: 1572992
I1126 19:38:40.722617 20696 layer_factory.hpp:77] Creating layer conv1
I1126 19:38:40.722635 20696 net.cpp:84] Creating Layer conv1
I1126 19:38:40.722641 20696 net.cpp:406] conv1 <- data
I1126 19:38:40.722652 20696 net.cpp:380] conv1 -> conv1
I1126 19:38:40.723445 20696 net.cpp:122] Setting up conv1
I1126 19:38:40.723464 20696 net.cpp:129] Top shape: 32 20 63 63 (2540160)
I1126 19:38:40.723470 20696 net.cpp:137] Memory required for data: 11733632
I1126 19:38:40.723484 20696 layer_factory.hpp:77] Creating layer sigmoid1
I1126 19:38:40.723495 20696 net.cpp:84] Creating Layer sigmoid1
I1126 19:38:40.723498 20696 net.cpp:406] sigmoid1 <- conv1
I1126 19:38:40.723518 20696 net.cpp:380] sigmoid1 -> sigmoid1
I1126 19:38:40.723551 20696 net.cpp:122] Setting up sigmoid1
I1126 19:38:40.723561 20696 net.cpp:129] Top shape: 32 20 63 63 (2540160)
I1126 19:38:40.723567 20696 net.cpp:137] Memory required for data: 21894272
I1126 19:38:40.723578 20696 layer_factory.hpp:77] Creating layer pool1
I1126 19:38:40.723598 20696 net.cpp:84] Creating Layer pool1
I1126 19:38:40.723603 20696 net.cpp:406] pool1 <- sigmoid1
I1126 19:38:40.723609 20696 net.cpp:380] pool1 -> pool1
I1126 19:38:40.723661 20696 net.cpp:122] Setting up pool1
I1126 19:38:40.723671 20696 net.cpp:129] Top shape: 32 20 32 32 (655360)
I1126 19:38:40.723676 20696 net.cpp:137] Memory required for data: 24515712
I1126 19:38:40.723680 20696 layer_factory.hpp:77] Creating layer conv2
I1126 19:38:40.723690 20696 net.cpp:84] Creating Layer conv2
I1126 19:38:40.723695 20696 net.cpp:406] conv2 <- pool1
I1126 19:38:40.723703 20696 net.cpp:380] conv2 -> conv2
I1126 19:38:40.725190 20696 net.cpp:122] Setting up conv2
I1126 19:38:40.725209 20696 net.cpp:129] Top shape: 32 40 29 29 (1076480)
I1126 19:38:40.725214 20696 net.cpp:137] Memory required for data: 28821632
I1126 19:38:40.725226 20696 layer_factory.hpp:77] Creating layer sigmoid2
I1126 19:38:40.725234 20696 net.cpp:84] Creating Layer sigmoid2
I1126 19:38:40.725239 20696 net.cpp:406] sigmoid2 <- conv2
I1126 19:38:40.725251 20696 net.cpp:380] sigmoid2 -> sigmoid2
I1126 19:38:40.725280 20696 net.cpp:122] Setting up sigmoid2
I1126 19:38:40.725287 20696 net.cpp:129] Top shape: 32 40 29 29 (1076480)
I1126 19:38:40.725292 20696 net.cpp:137] Memory required for data: 33127552
I1126 19:38:40.725296 20696 layer_factory.hpp:77] Creating layer pool2
I1126 19:38:40.725306 20696 net.cpp:84] Creating Layer pool2
I1126 19:38:40.725311 20696 net.cpp:406] pool2 <- sigmoid2
I1126 19:38:40.725317 20696 net.cpp:380] pool2 -> pool2
I1126 19:38:40.725352 20696 net.cpp:122] Setting up pool2
I1126 19:38:40.725359 20696 net.cpp:129] Top shape: 32 40 15 15 (288000)
I1126 19:38:40.725364 20696 net.cpp:137] Memory required for data: 34279552
I1126 19:38:40.725368 20696 layer_factory.hpp:77] Creating layer fc1
I1126 19:38:40.725376 20696 net.cpp:84] Creating Layer fc1
I1126 19:38:40.725380 20696 net.cpp:406] fc1 <- pool2
I1126 19:38:40.725388 20696 net.cpp:380] fc1 -> fc1
I1126 19:38:40.761747 20696 net.cpp:122] Setting up fc1
I1126 19:38:40.761776 20696 net.cpp:129] Top shape: 32 500 (16000)
I1126 19:38:40.761781 20696 net.cpp:137] Memory required for data: 34343552
I1126 19:38:40.761793 20696 layer_factory.hpp:77] Creating layer score
I1126 19:38:40.761806 20696 net.cpp:84] Creating Layer score
I1126 19:38:40.761811 20696 net.cpp:406] score <- fc1
I1126 19:38:40.761817 20696 net.cpp:380] score -> score
I1126 19:38:40.761925 20696 net.cpp:122] Setting up score
I1126 19:38:40.761934 20696 net.cpp:129] Top shape: 32 3 (96)
I1126 19:38:40.761940 20696 net.cpp:137] Memory required for data: 34343936
I1126 19:38:40.761945 20696 layer_factory.hpp:77] Creating layer loss
I1126 19:38:40.761953 20696 net.cpp:84] Creating Layer loss
I1126 19:38:40.761957 20696 net.cpp:406] loss <- score
I1126 19:38:40.761962 20696 net.cpp:406] loss <- label
I1126 19:38:40.761971 20696 net.cpp:380] loss -> loss
I1126 19:38:40.761991 20696 layer_factory.hpp:77] Creating layer loss
I1126 19:38:40.762073 20696 net.cpp:122] Setting up loss
I1126 19:38:40.762082 20696 net.cpp:129] Top shape: (1)
I1126 19:38:40.762087 20696 net.cpp:132]     with loss weight 1
I1126 19:38:40.762106 20696 net.cpp:137] Memory required for data: 34343940
I1126 19:38:40.762111 20696 net.cpp:198] loss needs backward computation.
I1126 19:38:40.762115 20696 net.cpp:198] score needs backward computation.
I1126 19:38:40.762120 20696 net.cpp:198] fc1 needs backward computation.
I1126 19:38:40.762125 20696 net.cpp:198] pool2 needs backward computation.
I1126 19:38:40.762130 20696 net.cpp:198] sigmoid2 needs backward computation.
I1126 19:38:40.762135 20696 net.cpp:198] conv2 needs backward computation.
I1126 19:38:40.762138 20696 net.cpp:198] pool1 needs backward computation.
I1126 19:38:40.762143 20696 net.cpp:198] sigmoid1 needs backward computation.
I1126 19:38:40.762147 20696 net.cpp:198] conv1 needs backward computation.
I1126 19:38:40.762151 20696 net.cpp:200] data does not need backward computation.
I1126 19:38:40.762163 20696 net.cpp:242] This network produces output loss
I1126 19:38:40.762183 20696 net.cpp:255] Network initialization done.
I1126 19:38:40.762449 20696 solver.cpp:172] Creating test net (#0) specified by net file: train_val.prototxt
I1126 19:38:40.762475 20696 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1126 19:38:40.762547 20696 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215689
  }
  data_param {
    source: "/home/styurin/DataSet/lmdb/val"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 20
    kernel_size: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "sigmoid1"
  type: "Sigmoid"
  bottom: "conv1"
  top: "sigmoid1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "sigmoid1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 40
    kernel_size: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "sigmoid2"
  type: "Sigmoid"
  bottom: "conv2"
  top: "sigmoid2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "sigmoid2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "fc1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "fc1"
  top: "score"
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
I1126 19:38:40.762619 20696 layer_factory.hpp:77] Creating layer data
I1126 19:38:40.762676 20696 db_lmdb.cpp:35] Opened lmdb /home/styurin/DataSet/lmdb/val
I1126 19:38:40.762698 20696 net.cpp:84] Creating Layer data
I1126 19:38:40.762706 20696 net.cpp:380] data -> data
I1126 19:38:40.762715 20696 net.cpp:380] data -> label
I1126 19:38:40.763058 20696 data_layer.cpp:45] output data size: 64,3,64,64
I1126 19:38:40.775741 20696 net.cpp:122] Setting up data
I1126 19:38:40.775761 20696 net.cpp:129] Top shape: 64 3 64 64 (786432)
I1126 19:38:40.775768 20696 net.cpp:129] Top shape: 64 (64)
I1126 19:38:40.775773 20696 net.cpp:137] Memory required for data: 3145984
I1126 19:38:40.775777 20696 layer_factory.hpp:77] Creating layer label_data_1_split
I1126 19:38:40.775790 20696 net.cpp:84] Creating Layer label_data_1_split
I1126 19:38:40.775795 20696 net.cpp:406] label_data_1_split <- label
I1126 19:38:40.775805 20696 net.cpp:380] label_data_1_split -> label_data_1_split_0
I1126 19:38:40.775813 20696 net.cpp:380] label_data_1_split -> label_data_1_split_1
I1126 19:38:40.775907 20696 net.cpp:122] Setting up label_data_1_split
I1126 19:38:40.775919 20696 net.cpp:129] Top shape: 64 (64)
I1126 19:38:40.775925 20696 net.cpp:129] Top shape: 64 (64)
I1126 19:38:40.775929 20696 net.cpp:137] Memory required for data: 3146496
I1126 19:38:40.775933 20696 layer_factory.hpp:77] Creating layer conv1
I1126 19:38:40.775951 20696 net.cpp:84] Creating Layer conv1
I1126 19:38:40.775957 20696 net.cpp:406] conv1 <- data
I1126 19:38:40.775964 20696 net.cpp:380] conv1 -> conv1
I1126 19:38:40.776182 20696 net.cpp:122] Setting up conv1
I1126 19:38:40.776195 20696 net.cpp:129] Top shape: 64 20 63 63 (5080320)
I1126 19:38:40.776198 20696 net.cpp:137] Memory required for data: 23467776
I1126 19:38:40.776208 20696 layer_factory.hpp:77] Creating layer sigmoid1
I1126 19:38:40.776216 20696 net.cpp:84] Creating Layer sigmoid1
I1126 19:38:40.776226 20696 net.cpp:406] sigmoid1 <- conv1
I1126 19:38:40.776232 20696 net.cpp:380] sigmoid1 -> sigmoid1
I1126 19:38:40.776264 20696 net.cpp:122] Setting up sigmoid1
I1126 19:38:40.776273 20696 net.cpp:129] Top shape: 64 20 63 63 (5080320)
I1126 19:38:40.776278 20696 net.cpp:137] Memory required for data: 43789056
I1126 19:38:40.776283 20696 layer_factory.hpp:77] Creating layer pool1
I1126 19:38:40.776291 20696 net.cpp:84] Creating Layer pool1
I1126 19:38:40.776295 20696 net.cpp:406] pool1 <- sigmoid1
I1126 19:38:40.776301 20696 net.cpp:380] pool1 -> pool1
I1126 19:38:40.776336 20696 net.cpp:122] Setting up pool1
I1126 19:38:40.776345 20696 net.cpp:129] Top shape: 64 20 32 32 (1310720)
I1126 19:38:40.776348 20696 net.cpp:137] Memory required for data: 49031936
I1126 19:38:40.776360 20696 layer_factory.hpp:77] Creating layer conv2
I1126 19:38:40.776372 20696 net.cpp:84] Creating Layer conv2
I1126 19:38:40.776377 20696 net.cpp:406] conv2 <- pool1
I1126 19:38:40.776383 20696 net.cpp:380] conv2 -> conv2
I1126 19:38:40.776764 20696 net.cpp:122] Setting up conv2
I1126 19:38:40.776778 20696 net.cpp:129] Top shape: 64 40 29 29 (2152960)
I1126 19:38:40.776783 20696 net.cpp:137] Memory required for data: 57643776
I1126 19:38:40.776793 20696 layer_factory.hpp:77] Creating layer sigmoid2
I1126 19:38:40.776801 20696 net.cpp:84] Creating Layer sigmoid2
I1126 19:38:40.776806 20696 net.cpp:406] sigmoid2 <- conv2
I1126 19:38:40.776813 20696 net.cpp:380] sigmoid2 -> sigmoid2
I1126 19:38:40.776837 20696 net.cpp:122] Setting up sigmoid2
I1126 19:38:40.776845 20696 net.cpp:129] Top shape: 64 40 29 29 (2152960)
I1126 19:38:40.776849 20696 net.cpp:137] Memory required for data: 66255616
I1126 19:38:40.776854 20696 layer_factory.hpp:77] Creating layer pool2
I1126 19:38:40.776862 20696 net.cpp:84] Creating Layer pool2
I1126 19:38:40.776866 20696 net.cpp:406] pool2 <- sigmoid2
I1126 19:38:40.776872 20696 net.cpp:380] pool2 -> pool2
I1126 19:38:40.778156 20696 net.cpp:122] Setting up pool2
I1126 19:38:40.778168 20696 net.cpp:129] Top shape: 64 40 15 15 (576000)
I1126 19:38:40.778172 20696 net.cpp:137] Memory required for data: 68559616
I1126 19:38:40.778177 20696 layer_factory.hpp:77] Creating layer fc1
I1126 19:38:40.778187 20696 net.cpp:84] Creating Layer fc1
I1126 19:38:40.778192 20696 net.cpp:406] fc1 <- pool2
I1126 19:38:40.778198 20696 net.cpp:380] fc1 -> fc1
I1126 19:38:40.813787 20696 net.cpp:122] Setting up fc1
I1126 19:38:40.813809 20696 net.cpp:129] Top shape: 64 500 (32000)
I1126 19:38:40.813814 20696 net.cpp:137] Memory required for data: 68687616
I1126 19:38:40.813827 20696 layer_factory.hpp:77] Creating layer score
I1126 19:38:40.813838 20696 net.cpp:84] Creating Layer score
I1126 19:38:40.813843 20696 net.cpp:406] score <- fc1
I1126 19:38:40.813849 20696 net.cpp:380] score -> score
I1126 19:38:40.813954 20696 net.cpp:122] Setting up score
I1126 19:38:40.813964 20696 net.cpp:129] Top shape: 64 3 (192)
I1126 19:38:40.813968 20696 net.cpp:137] Memory required for data: 68688384
I1126 19:38:40.813976 20696 layer_factory.hpp:77] Creating layer score_score_0_split
I1126 19:38:40.813982 20696 net.cpp:84] Creating Layer score_score_0_split
I1126 19:38:40.813985 20696 net.cpp:406] score_score_0_split <- score
I1126 19:38:40.813992 20696 net.cpp:380] score_score_0_split -> score_score_0_split_0
I1126 19:38:40.814000 20696 net.cpp:380] score_score_0_split -> score_score_0_split_1
I1126 19:38:40.814034 20696 net.cpp:122] Setting up score_score_0_split
I1126 19:38:40.814043 20696 net.cpp:129] Top shape: 64 3 (192)
I1126 19:38:40.814047 20696 net.cpp:129] Top shape: 64 3 (192)
I1126 19:38:40.814051 20696 net.cpp:137] Memory required for data: 68689920
I1126 19:38:40.814055 20696 layer_factory.hpp:77] Creating layer accuracy
I1126 19:38:40.814064 20696 net.cpp:84] Creating Layer accuracy
I1126 19:38:40.814069 20696 net.cpp:406] accuracy <- score_score_0_split_0
I1126 19:38:40.814074 20696 net.cpp:406] accuracy <- label_data_1_split_0
I1126 19:38:40.814080 20696 net.cpp:380] accuracy -> accuracy
I1126 19:38:40.814093 20696 net.cpp:122] Setting up accuracy
I1126 19:38:40.814105 20696 net.cpp:129] Top shape: (1)
I1126 19:38:40.814110 20696 net.cpp:137] Memory required for data: 68689924
I1126 19:38:40.814121 20696 layer_factory.hpp:77] Creating layer loss
I1126 19:38:40.814129 20696 net.cpp:84] Creating Layer loss
I1126 19:38:40.814134 20696 net.cpp:406] loss <- score_score_0_split_1
I1126 19:38:40.814139 20696 net.cpp:406] loss <- label_data_1_split_1
I1126 19:38:40.814144 20696 net.cpp:380] loss -> loss
I1126 19:38:40.814152 20696 layer_factory.hpp:77] Creating layer loss
I1126 19:38:40.814234 20696 net.cpp:122] Setting up loss
I1126 19:38:40.814244 20696 net.cpp:129] Top shape: (1)
I1126 19:38:40.814249 20696 net.cpp:132]     with loss weight 1
I1126 19:38:40.814257 20696 net.cpp:137] Memory required for data: 68689928
I1126 19:38:40.814261 20696 net.cpp:198] loss needs backward computation.
I1126 19:38:40.814266 20696 net.cpp:200] accuracy does not need backward computation.
I1126 19:38:40.814271 20696 net.cpp:198] score_score_0_split needs backward computation.
I1126 19:38:40.814275 20696 net.cpp:198] score needs backward computation.
I1126 19:38:40.814280 20696 net.cpp:198] fc1 needs backward computation.
I1126 19:38:40.814283 20696 net.cpp:198] pool2 needs backward computation.
I1126 19:38:40.814287 20696 net.cpp:198] sigmoid2 needs backward computation.
I1126 19:38:40.814292 20696 net.cpp:198] conv2 needs backward computation.
I1126 19:38:40.814296 20696 net.cpp:198] pool1 needs backward computation.
I1126 19:38:40.814301 20696 net.cpp:198] sigmoid1 needs backward computation.
I1126 19:38:40.814304 20696 net.cpp:198] conv1 needs backward computation.
I1126 19:38:40.814309 20696 net.cpp:200] label_data_1_split does not need backward computation.
I1126 19:38:40.814314 20696 net.cpp:200] data does not need backward computation.
I1126 19:38:40.814318 20696 net.cpp:242] This network produces output accuracy
I1126 19:38:40.814323 20696 net.cpp:242] This network produces output loss
I1126 19:38:40.814335 20696 net.cpp:255] Network initialization done.
I1126 19:38:40.814381 20696 solver.cpp:56] Solver scaffolding done.
I1126 19:38:40.814626 20696 caffe.cpp:248] Starting Optimization
I1126 19:38:40.814635 20696 solver.cpp:272] Solving 
I1126 19:38:40.814637 20696 solver.cpp:273] Learning Rate Policy: inv
I1126 19:38:40.815680 20696 solver.cpp:330] Iteration 0, Testing net (#0)
I1126 19:38:41.015204 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:41.330931 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:41.616955 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:41.914448 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:42.201942 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:42.493726 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:42.788868 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:43.091938 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:43.408833 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:43.688181 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:44.005374 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:44.320439 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:44.618093 20700 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:44.695911 20696 solver.cpp:397]     Test net output #0: accuracy = 0.656094
I1126 19:38:44.695948 20696 solver.cpp:397]     Test net output #1: loss = 0.8923 (* 1 = 0.8923 loss)
I1126 19:38:44.786270 20696 solver.cpp:218] Iteration 0 (0 iter/s, 3.97163s/100 iters), loss = 0.950643
I1126 19:38:44.786315 20696 solver.cpp:237]     Train net output #0: loss = 0.950643 (* 1 = 0.950643 loss)
I1126 19:38:44.786336 20696 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I1126 19:38:49.129803 20699 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:53.948340 20699 data_layer.cpp:73] Restarting data prefetching from start.
I1126 19:38:55.061101 20696 solver.cpp:218] Iteration 100 (9.73448 iter/s, 10.2728s/100 iters), loss = 1.04919
I1126 19:38:55.061188 20696 solver.cpp:237]     Train net output #0: loss = 1.04919 (* 1 = 1.04919 loss)
I1126 19:38:55.061203 20696 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I1126 19:38:56.241463 20696 solver.cpp:447] Snapshotting to binary proto file snapshots/classifier_iter_113.caffemodel
I1126 19:38:56.375540 20696 sgd_solver.cpp:273] Snapshotting solver state to binary proto file snapshots/classifier_iter_113.solverstate
I1126 19:38:56.398222 20696 solver.cpp:294] Optimization stopped early.
I1126 19:38:56.398241 20696 caffe.cpp:259] Optimization Done.
